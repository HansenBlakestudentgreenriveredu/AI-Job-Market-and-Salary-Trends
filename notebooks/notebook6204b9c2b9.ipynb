{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset from the local CSV file\n",
    "df = pd.read_csv(\"../data/ai_job_dataset.csv\")\n",
    "\n",
    "# Drop rows with missing salary\n",
    "df = df.dropna(subset=['salary_usd'])\n",
    "\n",
    "# Confirm environment\n",
    "print(\"Python executable in use:\", sys.executable)\n",
    "\n",
    "# Print dimensions (rows, columns)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Print column names\n",
    "print(\"Column names:\", df.columns)\n",
    "\n",
    "# Show the first few rows\n",
    "print(\"First few rows:\\n\", df.head()) \n",
    "\n",
    "# Checking for missing values and summing missing values per column\n",
    "print(\"Missing values per column:\\n\", df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove salary outliers (top/bottom 1% and 0.1%)\n",
    "q_low = df['salary_usd'].quantile(0.001)\n",
    "q_high = df['salary_usd'].quantile(0.99)\n",
    "df = df[(df['salary_usd'] >= q_low) & (df['salary_usd'] <= q_high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preparation\n",
    "\n",
    "# Create num_skills column\n",
    "df['num_skills'] = df['required_skills'].apply(lambda x: len(str(x).split(',')) if pd.notna(x) else 0)\n",
    "\n",
    "# *** Attempted to use all features ***\n",
    "# Combine with other features\n",
    "features = ['experience_level', 'employment_type', 'job_title', 'employee_residence',\n",
    "           'company_location', 'company_size', 'remote_ratio']\n",
    "\n",
    "# Create X and y\n",
    "X = df[features]\n",
    "y = df['salary_usd']\n",
    "\n",
    "# One-hot encode\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# *** Attempted to use the 2 strongest figures (Looked like 80% of results) ***\n",
    "# Use only the two strongest features\n",
    "#X = df[['years_experience', 'experience_level']]\n",
    "# One-hot encode experience_level only\n",
    "#X = pd.get_dummies(X, columns=['experience_level'], drop_first=True)\n",
    "\n",
    "# Selected top 5 features\n",
    "#selected_features = [\n",
    "#    'job_title',\n",
    "#    'experience_level',\n",
    "#    'years_experience',\n",
    "#    'company_size',\n",
    "#    'benefits_score'\n",
    "#]\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Models\n",
    "lin_model = LinearRegression().fit(X_train, y_train)\n",
    "rf_model = RandomForestRegressor(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "lin_preds = lin_model.predict(X_test)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "def evaluate_model(y_true, preds, model_name):\n",
    "    print(f\" {model_name} Evaluation\")\n",
    "    print(\"MAE :\", mean_absolute_error(y_true, preds))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, preds)))\n",
    "    print(\"R²  :\", r2_score(y_true, preds))\n",
    "    print(\"\")\n",
    "\n",
    "evaluate_model(y_test, lin_preds, \"Linear Regression\")\n",
    "evaluate_model(y_test, rf_preds, \"Random Forest Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Convert education_required to one-hot\n",
    "df = pd.get_dummies(df, columns=['education_required'], drop_first=True)\n",
    "\n",
    "# Create numerical feature: number of listed skills\n",
    "df['num_skills'] = df['required_skills'].fillna('').apply(lambda x: len(x.split(',')) if x else 0)\n",
    "\n",
    "# Droped unused long text columns to avoid noise\n",
    "df = df.drop(columns=['required_skills', 'job_id', 'posting_date', 'application_deadline'])\n",
    "\n",
    "# Keep useful numeric features\n",
    "extra_features = ['years_experience', 'job_description_length', 'benefits_score', 'num_skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, y_train)\n",
    "lin_preds = lin_model.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "# Fit Random Forest to get feature importances\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Plot feature importances\n",
    "importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values(ascending=True).plot(kind='barh', figsize=(10, 8))\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Random Forest \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Expanded parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500, 1000],\n",
    "    'max_depth': [10, 20, 40, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Grid search with 3-fold cross-validation\n",
    "grid = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_rf = grid.best_estimator_\n",
    "rf_preds_tuned = best_rf.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test, rf_preds_tuned, \"Tuned Random Forest (Expanded Grid)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Linear Model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "poly_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False),\n",
    "    Ridge(alpha=1)\n",
    ")\n",
    "\n",
    "poly_model.fit(X_train, y_train)\n",
    "poly_preds = poly_model.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test, poly_preds, \"Polynomial Ridge Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret model outputs \n",
    "def evaluate_model(y_true, preds, model_name):\n",
    "    print(f\" {model_name} Evaluation\")\n",
    "    print(\"MAE :\", mean_absolute_error(y_true, preds))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, preds)))\n",
    "    print(\"R²  :\", r2_score(y_true, preds))\n",
    "    print(\"\")\n",
    "\n",
    "evaluate_model(y_test, lin_preds, \"Linear Regression\")\n",
    "evaluate_model(y_test, rf_preds, \"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(df.describe())\n",
    "\n",
    "# Summary for all columns including categorical\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of salary_usd\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['salary_usd'], bins=50, kde=True)\n",
    "plt.title('Salary Distribution')\n",
    "plt.xlabel('Salary (USD)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categorial features\n",
    "categorical_features = ['experience_level', 'employment_type', 'company_size', 'remote_ratio']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df, x=feature, order=df[feature].value_counts().index)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of salary vs categorial features\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=feature, y='salary_usd', data=df)\n",
    "    plt.title(f'Salary vs {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of numerical features\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix (Numeric Features Only)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot for nymerical relationships\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='years_experience', y='salary_usd', data=df)\n",
    "plt.title('Salary vs Years of Experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model R^2 scores\n",
    "results = {\n",
    "    'Model': ['Linear Regression', 'Random Forest'],\n",
    "    'R² Score': [r2_score(y_test, lin_preds), r2_score(y_test, rf_preds)]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "sns.barplot(data=results_df, x='Model', y='R² Score', palette='Set2')\n",
    "plt.title('Model Comparison: R² Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Original and tuned scores\n",
    "data = {\n",
    "    'Model': ['Linear Regression', 'Random Forest'],\n",
    "    'MAE Before': [15953.41, 15164.81],\n",
    "    'MAE After': [14966.45, 14268.83],\n",
    "\n",
    "    'RMSE Before': [21010.50, 21035.32],\n",
    "    'RMSE After': [20032.77, 19513.56],\n",
    "\n",
    "    'R2 Before': [0.8561, 0.8558],\n",
    "    'R2 After': [0.8692, 0.8759]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate % improvement for error metrics (lower is better)\n",
    "df['MAE Improvement (%)'] = 100 * (df['MAE Before'] - df['MAE After']) / df['MAE Before']\n",
    "df['RMSE Improvement (%)'] = 100 * (df['RMSE Before'] - df['RMSE After']) / df['RMSE Before']\n",
    "# For R2, higher is better, so calculate increase\n",
    "df['R2 Improvement (%)'] = 100 * (df['R2 After'] - df['R2 Before']) / df['R2 Before']\n",
    "\n",
    "# Melt for plotting\n",
    "results_long = df.melt(id_vars='Model', value_vars=['MAE Improvement (%)', 'RMSE Improvement (%)', 'R2 Improvement (%)'],\n",
    "                       var_name='Metric', value_name='Improvement')\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=results_long, x='Metric', y='Improvement', hue='Model', palette='muted')\n",
    "\n",
    "plt.title('Percentage Improvement After Hyperparameter Tuning')\n",
    "plt.ylabel('Improvement (%)')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, results_long['Improvement'].max() * 1.2)  # Add some padding\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7564261,
     "sourceId": 12023018,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
